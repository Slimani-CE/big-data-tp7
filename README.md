#
## Partie 1 - Traitement de donnÃ©es stockÃ©es dans MySQL
### Les tables MySQL
#### La table TACHES
![img.png](assets/img.png)
#### La table PROJETS
![img.png](assets/img_1.png)
### Question 1 : Afficher les projets en cours de rÃ©alisation
#### Code Snippet ðŸ’»
```java
public class App {
    public static void main(String[] args) {
        // Connection SparkSession
        SparkSession spark = SparkSession.builder()
                .appName("Application")
                .master("local[*]")
                .getOrCreate();

        // Lecture de donnÃ©es utilisant JDBC
        Dataset<Row> dataframe = spark
                .read()
                .format("jdbc")
                .option("driver", "com.mysql.jdbc.Driver")
                .option("url", "jdbc:mysql://localhost:3306/db_imomaroc")
                .option("dbtable", "projets")
                .option("user", "root")
                .option("password", "")
                .load();

        // Affichage des projets encore de realisation
        dataframe.filter("date_fin > now()").show();
    }
}
```

#### RÃ©sultat âœ…
![img.png](assets/img_2.png)


### Question 2 : Afficher pour chaque projet, le nombre de tÃ¢ches dont la durÃ©e dÃ©passe un mois
#### Code Snippet ðŸ’»
```java
public class App {
    public static void main(String[] args) {
        // Connection SparkSession
        SparkSession spark = SparkSession.builder()
                .appName("Application")
                .master("local[*]")
                .getOrCreate();

        // Lecture de donnÃ©es utilisant JDBC (PROJETS)
        Dataset<Row> projets = spark
                .read()
                .format("jdbc")
                .option("driver", "com.mysql.jdbc.Driver")
                .option("url", "jdbc:mysql://localhost:3306/db_imomaroc")
                .option("query", "select id_projet, titre from projets")
                .option("user", "root")
                .option("password", "")
                .load();

        // Lecture de donnÃ©es utilisant JDBC (TACHES)
        Dataset<Row> taches = spark
                .read()
                .format("jdbc")
                .option("driver", "com.mysql.jdbc.Driver")
                .option("url", "jdbc:mysql://localhost:3306/db_imomaroc")
                .option("query", "select id_projet, date_debut, date_fin from taches")
                .option("user", "root")
                .option("password", "")
                .load();

        // Une pour chaque projet, le nombre de taches dont la durÃ©e dÃ©passe un mois
        // Le format de sortie est le suivant:
        // ID_PROJET | TITRE | NOMBRE
        projets.join(taches, "id_projet")
                .filter("datediff(date_fin, date_debut) > 30")
                .groupBy("id_projet", "titre")
                .count()
                .show();
        // Fermeture de la session Spark
        spark.close();
    }
}
```

#### RÃ©sultat âœ…
![img.png](assets/img_3.png)

### Question 3 : Afficher pour chaque projet les tÃ¢ches en retard

#### Code Snippet ðŸ’»
```java
public class App {
    public static void main(String[] args) {
        // Connection SparkSession
        SparkSession spark = SparkSession.builder()
                .appName("Application")
                .master("local[*]")
                .getOrCreate();

        // Lecture de donnÃ©es utilisant JDBC (PROJETS)
        Dataset<Row> projets = spark
                .read()
                .format("jdbc")
                .option("driver", "com.mysql.jdbc.Driver")
                .option("url", "jdbc:mysql://localhost:3306/db_imomaroc")
                .option("query", "select * from projets")
                .option("user", "root")
                .option("password", "")
                .load();

        // Lecture de donnÃ©es utilisant JDBC (TACHES)
        Dataset<Row> taches = spark
                .read()
                .format("jdbc")
                .option("driver", "com.mysql.jdbc.Driver")
                .option("url", "jdbc:mysql://localhost:3306/db_imomaroc")
                .option("query", "select * from taches")
                .option("user", "root")
                .option("password", "")
                .load();

        Dataset<Row> joined = projets.join(taches, projets.col("ID_PROJET").equalTo(taches.col("ID_PROJET")));

        // Filter the tasks that are not finished and are delayed
        Dataset<Row> delayedTasks = joined.filter(taches.col("TERMINE").equalTo(0)
                .and(taches.col("DATE_FIN").lt(current_date())));

        // Select the necessary fields
        Dataset<Row> result = delayedTasks.select(projets.col("ID_PROJET"), projets.col("TITRE").as("PROJET_TITRE"),
                taches.col("TITRE").as("TACHE_TITRE"), taches.col("DATE_DEBUT"), taches.col("DATE_FIN"),
                datediff(current_date(), taches.col("DATE_FIN")).as("RETARD"));

        // Display the result
        result.show();

        // Close the Spark session
        spark.close();
    }
}
```

#### RÃ©sultat âœ…
![img.png](assets/img_4.png)

## Partie 2 - Traitement de donnÃ©es stockÃ©es dans un fichier Json (Avec PySpark âœ¨)

### Example des donnÃ©es
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()
df = spark.read.option("multiline", True).json('ventes.json')
df.show()
```
![img_9](assets/img_9.png)

### 1. Afficher les deux premiÃ¨res villes oÃ¹ lâ€™entreprise a rÃ©alisÃ© le plus de ventes (en termes de revenus)

#### Code Snippet ðŸ’»(PySpark)
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()
df = spark.read.option("multiline", True).json('ventes.json')

df.groupBy('ville').agg({'revenues': 'sum'}).orderBy('sum(revenues)', ascending=False).limit(2).show()

```

#### RÃ©sultat âœ…
![img_5](assets/img_6.png)

### 2. Afficher la liste des clients qui possÃ¨dent plus quâ€™un appartement Ã  rabat.
#### Code Snippet ðŸ’»(PySpark)
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()
df = spark.read.option("multiline", True).json('ventes.json')

df.filter(df.ville == 'Rabat').groupBy('nom_client').count().filter('count > 1').show()
```

#### RÃ©sultat âœ…
![img_6](assets/img_6.png)

### 3. Afficher le nombre dâ€™appartements vendues Ã  Casablanca.
#### Code Snippet ðŸ’»(PySpark)
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()
df = spark.read.option("multiline", True).json('ventes.json')

df.filter(df.ville == 'Casablanca').groupBy('nom_client').count().show()
```

#### RÃ©sultat âœ…
![img_6](assets/img_6.png)